{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIGdp5qIAZWyW/AaoVUqY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alkarps/GB.Methods-for-collecting-and-processing-data-from-the-Internet/blob/hw03/hw03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FPB4nq6Y_8sa"
      },
      "outputs": [],
      "source": [
        "#!pip install bs4\n",
        "#!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "from pprint import pprint\n",
        "from time import sleep\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "_vmmDHxQADiN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def saveJsonTo(file_name, income):\n",
        "  json_string = json.dumps(income)\n",
        "  with open(file_name, 'w') as outfile:\n",
        "      outfile.write(json_string)\n",
        "\n",
        "def readAndPrintJsonFrom(file_name):\n",
        "  with open(file_name) as json_file:\n",
        "    data = json.load(json_file)\n",
        "    print(data)"
      ],
      "metadata": {
        "id": "H6PjY5UnNvBB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vacancy:\n",
        "    def __init__(self, source, name, link, min_salary = None, max_salary = None):\n",
        "        self.source = source\n",
        "        self.name = name\n",
        "        self.link = link\n",
        "        self.min_salary = min_salary\n",
        "        self.max_salary = max_salary\n",
        "    def __str__(self):\n",
        "        return f\"Vacancy(source={self.source}; name={self.name}; link={self.link}; min_salary={self.min_salary}; max_salary={self.max_salary})\"\n",
        "\n",
        "class Salary:\n",
        "    def __init__(self, amount, currency):\n",
        "        self.amount = amount\n",
        "        self.currency = currency\n",
        "    def __str__(self):\n",
        "        return f\"Salary(amount={self.amount}; currency={self.currency})\""
      ],
      "metadata": {
        "id": "ujrefdN1FNUw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soup_by_url(url, payload):\n",
        "  res = requests.get(url, params=payload)\n",
        "  return bs(res.content, 'html.parser')\n",
        "\n",
        "def get_hh_ru_soup(vacancy_name, page):\n",
        "  return get_soup_by_url(\"https://hh.ru/search/vacancy\", {\"text\": vacancy_name, 'page': page, 'hhtmFrom': 'vacancy_search_list'})"
      ],
      "metadata": {
        "id": "-ZqEYIgKHaNO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Собрать информацию о вакансиях на вводимую должность с сайтов hh.ru, SuperJob и/или работа.ру. Приложение должно анализировать несколько страниц сайта. Полученный список должен содержать в себе минимум:**"
      ],
      "metadata": {
        "id": "ppOgQ7rRASa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Наименование вакансии.\n",
        "*   Предполагаемую зарплату (**дополнительно**: разносим на три поля: минимульную, максимальную и валюту. Цифры приводим к числовому формату). \n",
        "*   Ссылку на саму вакансию.\n",
        "*   Сайт, откуда собрана вакансия.\n"
      ],
      "metadata": {
        "id": "BwGE6ouwAzns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "По желанию, можно добавить еще параметров (например, работодатели в предложении). Структура должна быть одинакова для ванаксий со всех сайтов. Общий результат можно вывести в помощью dataFrame, сохранить в json или csv формат."
      ],
      "metadata": {
        "id": "Rh9NAYuLBelR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_hh_page(soup):\n",
        "  result = list()\n",
        "  source = \"hh.ru\"\n",
        "  vacancy_cards = soup.find_all(\"div\", {\"class\": \"vacancy-serp-item-body__main-info\"})\n",
        "  for vacancy_card in vacancy_cards:\n",
        "    vacancy = vacancy_card.find(\"a\", {\"class\": \"serp-item__title\"})\n",
        "    link = vacancy[\"href\"]\n",
        "    title = vacancy.string\n",
        "    salary = vacancy_card.find(\"span\",{\"class\": \"bloko-header-section-3\"})\n",
        "    min_salary = None\n",
        "    max_salary = None\n",
        "    if salary: \n",
        "      salary = salary.string.split(\"<!-- -->\")\n",
        "      if salary.startswith(\"от\"):\n",
        "        min_salary = Salary(salary[1].split(), salary[3])\n",
        "      elif salary.startswith(\"до\"):\n",
        "        max_salary = Salary(salary[1].split(), salary[3])\n",
        "      else:\n",
        "        currency = salary[1]\n",
        "        min_salary = Salary(salary[0].split(\" - \")[0].split(), currency)\n",
        "        max_salary = Salary(salary[0].split(\" - \")[1].split(), currency)\n",
        "    result.append(Vacancy(source, title, link, min_salary, max_salary))\n",
        "  return result"
      ],
      "metadata": {
        "id": "L1yGscpkB5AA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_hh(vacancy_name):\n",
        "  result = list()\n",
        "  for i in range(40):\n",
        "    soup = get_hh_ru_soup(vacancy_name, str(i))\n",
        "    vacancies = parse_hh_page(soup)\n",
        "    result.extend(vacancies)\n",
        "  return result"
      ],
      "metadata": {
        "id": "octB1tLACrax"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vanacies = parse_hh(\"тестер\")\n",
        "saveJsonTo(\"parsed_vacancies.json\", vanacies)\n",
        "readAndPrintJsonFrom(\"parsed_vacancies.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKzEIWY5OANh",
        "outputId": "d111dfec-df6f-462f-ddee-90577cab2a80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    }
  ]
}